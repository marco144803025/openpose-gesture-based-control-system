import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.lines import Line2D
import os
import tensorflow
from tensorflow import keras
import pathlib
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path
import json
from sklearn.metrics import *
from scipy.special import softmax

def get_shape(lst):
    shape = []
    while isinstance(lst, list):
        shape.append(len(lst))
        lst = lst[0] if lst else None
    return shape

def dataAugmentation(
    x: np.ndarray,
    y: np.ndarray,
    augmentation_ratio: float = 0.5,
    remove_specific_keypoints: list = None,
    remove_rand_keypoints_nbr: int = None,
    random_noise_standard_deviation: float = None,
    scaling_factor: float = None,
    rotation_angle: float = None,
    scaling_factor_standard_deviation: float = None,
    rotation_angle_standard_deviation: float = None,
):

    """This function adds entries in the dataset by applying several data augmentation techniques
    depending on the arguments that are given.

    Args:
        x (np.ndarray): Dataset of entries for the neural network, works with either BODY25 or BODY18
        y (np.ndarray): Labels, one per entry
        augmentation_ratio (float, optional): The given float will be proportion of entries of the dataset that will be created by the
        data augmentation function. Defaults to .5.
        remove_specific_keypoints (list, optional): Remove keypoints indicated in the given list. Defaults to None.
        remove_rand_keypoints_nbr (int, optional): Remove the given number of keypoints randomly for each entry. Defaults to None.
        random_noise_standard_deviation (float, optional): Add noise for each keypoint following a normal distribution of
        the given standard deviation. Defaults to None.
        scaling_factor (float, optional): Scale every keypoint by the given scaling factor. Defaults to None.
        rotation_angle (float, optional): Rotate every keypoint by the given rotating angle. Defaults to None.
        scaling_factor_standard_deviation (float, optional): Scale each keypoint by a different scaling factor
        generated by a normal distribution of the given standard deviation. Defaults to None.
        rotation_angle_standard_deviation (float, optional): Rotate each keypoint by a different rotation angle
        generated by a normal distribution of the given standard deviation. Defaults to None.

    Returns:
        tuple(np.ndarray, np.ndarray): returns all the created entries and the labels associated
    """

    size_dataset, number_keypoints, *_ = x.shape
    number_keypoints=int(number_keypoints/2)
    # Number of entries that will be created
    number_entries_to_create = int(size_dataset * augmentation_ratio)

    # Where is stored newly created entries
    new_x = []
    new_y = []

    # Shuffle the entries
    shuffler = np.random.permutation(size_dataset)
    x = x[shuffler]
    y = y[shuffler]

    index_dataset = 0

    # Go through each entry one by one
    while number_entries_to_create > 0:

        entry = []

        # The scaling factor that will be used for this entry
        if type(scaling_factor_standard_deviation) != type(None):
            scaling_factor_random = np.random.normal(
                1, scaling_factor_standard_deviation
            )

        # The rotation angle that will be used for this entry
        if type(rotation_angle_standard_deviation) != type(None):
            rotation_angle_random = np.random.normal(
                0, rotation_angle_standard_deviation
            )

        # The loist of keypoints that will be removed for this entry
        if type(remove_rand_keypoints_nbr) != type(None):
            list_random_keypoints = [
                np.random.randint(0, number_keypoints)
                for i in range(remove_rand_keypoints_nbr)

            ]

        # Go through the keypoints of the entry
        for i in range(number_keypoints):
            index = i * 2  # Adjust the index based on the keypoint number and the alternating x-y format
            keypoint_x = x[index_dataset][index]
            keypoint_y = x[index_dataset][index + 1]
            
            # Apply normal noise
            if type(random_noise_standard_deviation) != type(None):
                keypoint_x += np.random.normal(0, random_noise_standard_deviation)
                keypoint_y += np.random.normal(0, random_noise_standard_deviation)

            # Apply the scaling faction
            if type(scaling_factor) != type(None):
                keypoint_x *= scaling_factor
                keypoint_y *= scaling_factor
            if type(scaling_factor_standard_deviation) != type(None):
                keypoint_x *= scaling_factor_random
                keypoint_y *= scaling_factor_random

            # Apply the rotation
            if type(rotation_angle) != type(None):
                theta = np.radians(rotation_angle)
                c, s = np.cos(theta), np.sin(theta)
                rotation_matrix = np.array(((c, -s), (s, c)))
                keypoint = np.array([keypoint_x, keypoint_y])
                rotated_keypoint = np.dot(rotation_matrix, keypoint)
                keypoint_x = rotated_keypoint[0]
                keypoint_y = rotated_keypoint[1]
            if type(rotation_angle_standard_deviation) != type(None):
                theta = np.radians(rotation_angle_random)
                c, s = np.cos(theta), np.sin(theta)
                rotation_matrix = np.array(((c, -s), (s, c)))
                keypoint = np.array([keypoint_x, keypoint_y])
                rotated_keypoint = np.dot(rotation_matrix, keypoint)
                keypoint_x = rotated_keypoint[0]
                keypoint_y = rotated_keypoint[1]

            # Remove the points
            if type(remove_rand_keypoints_nbr) != type(None):
                if i in list_random_keypoints:
                    keypoint_x = 0.0
                    keypoint_y = 0.0
            if type(remove_specific_keypoints) != type(None):
                if i in remove_specific_keypoints:
                    keypoint_x = 0.0
                    keypoint_y = 0.0
            # Add additionnal augmentation features
            entry.append(keypoint_x)
            entry.append(keypoint_y)

        new_x.append(entry)
        new_y.append(y[index_dataset])

        # If the augmentation_ratio is more than 1, after going through the whole
        # dataset, it will start over
        index_dataset = (index_dataset + 1) % size_dataset

        number_entries_to_create -= 1

    new_x = np.array(new_x)
    new_y = np.array(new_y)

    return (new_x, new_y)

def handDataset( 
    testSplit: float = 0.1,
    shuffle: bool = True,
    handID: int = 0,
    local_import: bool = False,
):
    """Return the dataset of hand keypoints (see pose_classification_kit/datasets/HandPose_Dataset.csv)
    as numpy arrays.

    Args:
        testSplit (float, optional): Percent of the dataset reserved for testing. Defaults to 0.15. Must be between 0.0 and 1.0.
        shuffle (bool, optional): Shuffle the whole dataset. Defaults to True.
        handID (int, optional): Select hand side - 0:left, 1:right. Default to 0.
        local_import (bool, optional): Choose to use local dataset or fetch online dataset (global repository). Default False.

    Returns:
        dict: {
        'x_train': training keypoints,
        'y_train': training labels,
        'y_train_onehot': training labels one-hot encoded,
        'x_test': testing keypoints,
        'y_test': testing labels,
        'y_test_onehot': testing labels one-hot encoded,
        'labels': list of labels
    }
    """
    assert 0.0 <= testSplit <= 1.0
    
    datasetPath = pathlib.Path(".").resolve()/"train"/ "HandPose7_Dataset.csv"
    dataset_df = pd.read_csv(datasetPath)


    hand_label = "right" if handID else "left"
    handLabels_df = {
        hand_i: dataset_df.loc[dataset_df["hand"] == hand_i].groupby("label")
        for hand_i in ["left", "right"]
    }
    labels = list(dataset_df.label.unique())

    # Find the minimum number of samples accross categories to uniformly distributed sample sets
    total_size_cat = handLabels_df[hand_label].size().min()
    test_size_cat = int(total_size_cat * testSplit)
    train_size_cat = total_size_cat - test_size_cat

    x_train = []
    x_test = []
    y_train = []
    y_test = []

    # Iterate over each labeled group
    for label, group in handLabels_df[hand_label]:
        # remove irrelevant columns
        group_array = group.drop(["label", "hand", "accuracy"], axis=1).to_numpy()
        np.random.shuffle(group_array)

        x_train.append(group_array[:train_size_cat])
        y_train.append([label] * train_size_cat)
        x_test.append(group_array[train_size_cat : train_size_cat + test_size_cat])
        y_test.append([label] * test_size_cat)

    # Concatenate sample sets as numpy arrays
    x_train = np.concatenate(x_train, axis=0)
    x_test = np.concatenate(x_test, axis=0)
    y_train = np.concatenate(y_train, axis=0)
    y_test = np.concatenate(y_test, axis=0)

    # Shuffle in unison
    if shuffle:
        shuffler_test = np.random.permutation(test_size_cat * len(labels))
        shuffler_train = np.random.permutation(train_size_cat * len(labels))
        x_train = x_train[shuffler_train]
        x_test = x_test[shuffler_test]
        y_train = y_train[shuffler_train]
        y_test = y_test[shuffler_test]

    # One-hot encoding
    y_train_onehot = get_one_hot(
        np.array([labels.index(sample) for sample in y_train]), len(labels)
    )
    y_test_onehot = get_one_hot(
        np.array([labels.index(sample) for sample in y_test]), len(labels)
    )

    return {
        "x_train": x_train,
        "y_train": y_train,
        "y_train_onehot": y_train_onehot,
        "x_test": x_test,
        "y_test": y_test,
        "y_test_onehot": y_test_onehot,
        "labels": np.array(labels),
    }    
    
def get_one_hot(targets: np.ndarray, nb_classes: int):
    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]
    return res.reshape(list(targets.shape) + [nb_classes])



dataset = handDataset(shuffle=True, handID=0)
x_train = dataset['x_train']
x_test = dataset['x_test']
y_train = dataset['y_train_onehot']
y_test = dataset['y_test']
y_test_onehot = dataset['y_test_onehot']
labels = dataset['labels']

print('the label is', labels)

x, y = [x_train], [y_train]

x[len(x):],y[len(y):] = tuple(zip(dataAugmentation(
    x_train, y_train,
    remove_rand_keypoints_nbr=1,
    scaling_factor_standard_deviation=.08,
    augmentation_ratio =0.33,
)))

x[len(x):],y[len(y):] = tuple(zip(dataAugmentation(
    x_train, y_train,
    rotation_angle=10,
    remove_rand_keypoints_nbr=1,
    random_noise_standard_deviation=.08,
    augmentation_ratio =0.33,
)))



x_train = np.concatenate(x, axis=0)
y_train = np.concatenate(y, axis=0)

print('the augmented shape is ',x_train.shape, y_train.shape)

model_train_history = {}
input_dim = x_train.shape[1]
validation_split = 0.20
epochs = 40

model = keras.models.Sequential(name = 'ANN-3x16',
    layers =
    [
        keras.layers.InputLayer(input_shape=input_dim),
        keras.layers.Dense(16, activation=keras.activations.relu),
        keras.layers.Dense(16, activation=keras.activations.relu),
        keras.layers.Dense(16, activation=keras.activations.relu),
        keras.layers.Dense(len(labels), activation=keras.activations.softmax),
    ]
)

model.summary()
model.compile(
    optimizer=keras.optimizers.Adam(),
    loss='categorical_crossentropy',
    metrics=['accuracy'], 
)

model_train_history[model] = model.fit(
    x=x_train,
    y=y_train,
    epochs=epochs,
    batch_size=4,
    validation_split=validation_split,
    shuffle=True,
    verbose=1,
)
# Evaluate model performance on test set

y_pred = model.predict(x_test)
max_indices = np.argmax(y_pred, axis=1)
# Create an array of zeros and assign 1 to the maximum value's index in each row
y_pred_softmax = np.zeros_like(y_pred)
y_pred_softmax[np.arange(len(y_pred)), max_indices] = 1

print("Resulting array:")
print(y_pred_softmax)
f1 = f1_score(y_test_onehot, y_pred_softmax,average="micro")  # Micro-averaging for multi-class problems
recall = recall_score(y_test_onehot, y_pred_softmax,average="micro")
precision = precision_score(y_test_onehot, y_pred_softmax,average="micro")

# Print evaluation metrics
print("F1-score:", f1)
print("Recall:", recall)
print("Precision:", precision)
# Create a figure and axis for the plot
fig, ax = plt.subplots(figsize=(8, 6))
for model, history in model_train_history.items():
    accuracy = history.history['accuracy']
    val_accuracy = history.history['val_accuracy']
    epochs = range(1, len(accuracy) + 1)
    
    ax.plot(epochs, accuracy, label=f'{model} Training Accuracy')
    ax.plot(epochs, val_accuracy, label=f'{model} Validation Accuracy')

# Set the plot title and labels
ax.set_title('Accuracy of Models')
ax.set_xlabel('Epochs')
ax.set_ylabel('Accuracy')

# Add a legend
ax.legend()

# Show the plot
plt.show()

save_path = pathlib.Path(".").resolve() / "model" /"foreval"/ "left_eval.h5"
model.save(save_path)
